{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "264f4486-a777-40c7-a190-60d2486c9237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import random, os\\ndef set_seed(seed=42):\\n    random.seed(seed)\\n    np.random.seed(seed)\\n    torch.manual_seed(seed)\\n    torch.cuda.manual_seed_all(seed)\\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\\n    torch.backends.cudnn.deterministic = True\\n    torch.backends.cudnn.benchmark = False\\n\\nset_seed(42)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Imports ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Ensure reproducibility (optional)\n",
    "\"\"\"import random, os\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1b63801-b2a8-4ffd-a2b8-62721e06629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_ibm = pd.read_csv(\"data/ibm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aecd154b-a35e-490e-bb98-c7a0b9e341fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stock(df_raw, patch_len, train_frac, val_frac):\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # turn column to datetime, add helper day column\n",
    "    df[\"Date Time\"] = pd.to_datetime(df[\"Date Time\"])\n",
    "    df = df.sort_values(\"Date Time\")\n",
    "    df[\"day\"] = df[\"Date Time\"].dt.date\n",
    "\n",
    "    # minutely log returns withing days from close\n",
    "    # fill first minute with 0 returns\n",
    "    df[\"log_close\"] = np.log(df[\"Close\"].astype(float))\n",
    "    df[\"ret_1m\"] = df.groupby(\"day\")[\"log_close\"].diff().fillna(0.0)\n",
    "\n",
    "    # to check if each days actually has 381 return minutes\n",
    "    day_return_count = df.groupby(\"day\")[\"ret_1m\"].size()\n",
    "    print(day_return_count.value_counts())\n",
    "    bars_mode = int(day_return_count.mode().iat[0]) # get mode of return minutes across dataset\n",
    "\n",
    "    # compute daily RV\n",
    "    daily = (df.groupby(\"day\")[\"ret_1m\"]\n",
    "               .agg(rv=lambda x: np.sum(x**2))\n",
    "               .reset_index())\n",
    "    \n",
    "    # in case there is a zero RV, replace it with small value before log\n",
    "    daily[\"log_rv\"] = np.log(daily[\"rv\"].replace(0.0, 1e-12))\n",
    "    # label for inputs from day d is log_rv of day d+1\n",
    "    daily[\"log_rv_tplus1\"] = daily[\"log_rv\"].shift(-1)\n",
    "    \n",
    "    # create binary filter for days without valid target (prob just last day)\n",
    "    valid_days = daily.dropna(subset=[\"log_rv_tplus1\"])[\"day\"]\n",
    "    # remove them from both minute_df and daily_df\n",
    "    df = df[df[\"day\"].isin(valid_days)].copy()\n",
    "    daily = daily[daily[\"day\"].isin(valid_days)].reset_index(drop=True)\n",
    "\n",
    "    # create minute index in each day\n",
    "    df[\"idx_in_day\"] = df.groupby(\"day\").cumcount()\n",
    "\n",
    "    # flag if we want to patch\n",
    "    if patch_len == 1:\n",
    "        # No patching: 1-minute tokens\n",
    "        df[\"patch_id\"] = df[\"idx_in_day\"]\n",
    "    # patch index inside day\n",
    "    else:\n",
    "        df[\"patch_id\"] = (df[\"idx_in_day\"] // patch_len).astype(int)\n",
    "\n",
    "    # number of full patches per day (patches with len() == patch_len)\n",
    "    tokens_full = (bars_mode // patch_len) if patch_len > 1 else bars_mode\n",
    "\n",
    "     # collect aggregate information for each patch\n",
    "    g = df.groupby([\"day\", \"patch_id\"])\n",
    "    patch = g.agg(\n",
    "        r_sum=(\"ret_1m\", \"sum\"), # summed returns\n",
    "        r_abs=(\"ret_1m\", lambda x: np.abs(x).sum()), # summed absolute returns\n",
    "        r_sq =(\"ret_1m\", lambda x: np.sum(x**2)), # summed squared returns\n",
    "        hi   =(\"High\", \"max\"), # high\n",
    "        lo   =(\"Low\", \"min\"), # low\n",
    "        vol  =(\"Volume\", \"sum\"), # volumne\n",
    "        r_last=(\"ret_1m\", \"last\"), # last return\n",
    "        n    =(\"ret_1m\", \"size\"), # number of values (should be patch_len)\n",
    "    ).reset_index()\n",
    "\n",
    "    # drop the last not filled token\n",
    "    patch = patch[patch[\"patch_id\"] < tokens_full].copy()\n",
    "\n",
    "    # robust log range\n",
    "    # patch[\"range_hl\"] = np.log(patch[\"hi\"] / patch[\"lo\"].replace(0, np.nan)).replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "    # patch = patch.drop(columns=[\"hi\", \"lo\"])\n",
    "\n",
    "\n",
    "\n",
    "    # positional encoding\n",
    "    \n",
    "    # relative time-of-day encoding\n",
    "    if tokens_full > 1:\n",
    "        patch[\"pos\"] = patch[\"patch_id\"] / (tokens_full - 1)\n",
    "    else:\n",
    "        patch[\"pos\"] = 0.0\n",
    "    patch[\"pos_sin\"] = np.sin(2 * np.pi * patch[\"pos\"])\n",
    "    patch[\"pos_cos\"] = np.cos(2 * np.pi * patch[\"pos\"])\n",
    "\n",
    "    # calendar encoding\n",
    "    # create small calendar daily index\n",
    "    cal = patch[[\"day\"]].drop_duplicates().copy()\n",
    "    cal_dt = pd.to_datetime(cal[\"day\"])\n",
    "    \n",
    "    # Day of Week\n",
    "    dow = cal_dt.dt.weekday\n",
    "    cal[\"dow_sin\"] = np.sin(2 * np.pi * dow / 7.0)\n",
    "    cal[\"dow_cos\"] = np.cos(2 * np.pi * dow / 7.0)\n",
    "    \n",
    "    # Day of Month\n",
    "    dom = cal_dt.dt.day\n",
    "    cal[\"dom_sin\"] = np.sin(2 * np.pi * dom / 31.0)\n",
    "    cal[\"dom_cos\"] = np.cos(2 * np.pi * dom / 31.0)\n",
    "    \n",
    "    # Month of Year\n",
    "    moy = cal_dt.dt.month\n",
    "    cal[\"moy_sin\"] = np.sin(2 * np.pi * moy / 12.0)\n",
    "    cal[\"moy_cos\"] = np.cos(2 * np.pi * moy / 12.0)\n",
    "\n",
    "    # Month-end flag (useful for rebalancing effects)\n",
    "    cal[\"is_month_end\"] = cal_dt.dt.is_month_end.astype(np.int8)\n",
    "\n",
    "    # Attach calendar features to every patch in that day\n",
    "    patch = patch.merge(cal, on=\"day\", how=\"left\")\n",
    "\n",
    "    feat_cols = [\n",
    "        \"r_sum\", \"r_abs\", \"r_sq\", \"vol\", \"r_last\",     # patch stats\n",
    "        \"pos_sin\", \"pos_cos\",                                      # time-of-day\n",
    "        \"dow_sin\", \"dow_cos\", \"dom_sin\", \"dom_cos\", \"moy_sin\", \"moy_cos\",  # calendar\n",
    "        \"is_month_end\",                                            # flag (0/1)\n",
    "    ]\n",
    "\n",
    "\n",
    "    # ------------ BUILD X\n",
    "    # get unique trading days, patch has one row for each patch\n",
    "    day_list = sorted(patch[\"day\"].unique())\n",
    "    # initialize lists\n",
    "    X_days, y_days = [], []\n",
    "    # map each day to its target in a dict\n",
    "    label_map = dict(zip(daily[\"day\"], daily[\"log_rv_tplus1\"]))\n",
    "\n",
    "    # loop over trading days\n",
    "    for d in day_list:\n",
    "        # get all patches in one day sorted by time\n",
    "        day_p = patch[(patch[\"day\"] == d)].sort_values(\"patch_id\")\n",
    "        # skip days with not full patches\n",
    "        if day_p.shape[0] != tokens_full:\n",
    "            continue\n",
    "        # turn features into numpy array of [Patch_len, n_features]\n",
    "        # each Xi is a matrix representation of one trading day\n",
    "        Xi = day_p[feat_cols].to_numpy(dtype=np.float32)            # [T_patches, d_features]\n",
    "        # look up target for that day from dict\n",
    "        yi = float(label_map[d])                                    # scalar target (log RV)\n",
    "        X_days.append(Xi)\n",
    "        y_days.append(yi)\n",
    "\n",
    "    # stack data\n",
    "    X = np.stack(X_days, axis=0)            # [N_days, T_tokens, d_in]\n",
    "    y = np.array(y_days, dtype=np.float32)  # [N_days]\n",
    "\n",
    "\n",
    "\n",
    "    # ------------- Train - Val - Test - Split\n",
    "    # number of trading days in data set\n",
    "    N = X.shape[0]\n",
    "    # train split\n",
    "    n_tr = int(N * train_frac)\n",
    "    # val split\n",
    "    n_va = int(N * val_frac)\n",
    "\n",
    "    # comput split indices\n",
    "    idx_tr = slice(0, n_tr)\n",
    "    idx_va = slice(n_tr, n_tr + n_va)\n",
    "    idx_te = slice(n_tr + n_va, N)\n",
    "\n",
    "    # split data\n",
    "    X_train, y_train = X[idx_tr], y[idx_tr]\n",
    "    X_val, y_val = X[idx_va], y[idx_va]\n",
    "    X_test, y_test = X[idx_te], y[idx_te]\n",
    "\n",
    "    # ------------------- Scaling\n",
    "\n",
    "    # only use train data for mean and sd\n",
    "    # flattens across days and tokens\n",
    "    mu = X_train.reshape(-1, X_train.shape[-1]).mean(axis=0, keepdims=True)   # [1, d_in], mean of each feature\n",
    "    sd = X_train.reshape(-1, X_train.shape[-1]).std(axis=0, keepdims=True) + 1e-8 # sd of each feature\n",
    "\n",
    "    def scale(arr):\n",
    "        return (arr - mu) / sd\n",
    "\n",
    "    X_train = scale(X_train)\n",
    "    X_val = scale(X_val)\n",
    "    X_test = scale(X_test)\n",
    "    \n",
    "    return {\n",
    "        \"X_train\": X_train, \"y_train\": y_train,\n",
    "        \"X_val\":   X_val, \"y_val\":   y_val,\n",
    "        \"X_test\":  X_test, \"y_test\":  y_test,\n",
    "        \"meta\": {\n",
    "            \"tokens_per_day\": tokens_full,\n",
    "            \"feature_names\": feat_cols,\n",
    "            \"bars_per_day_mode\": bars_mode,\n",
    "            \"train_days\": int(n_tr),\n",
    "            \"val_days\": int(n_va),\n",
    "            \"test_days\": int(N - n_tr - n_va),\n",
    "            \"scaler_mean\": mu.astype(np.float32),\n",
    "            \"scaler_std\": sd.astype(np.float32),\n",
    "            \"patch_len_minutes\": patch_len,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "281a1a2e-d3b9-4627-8c9c-18837846e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_context_days(X_days, y_days, context_window=22):\n",
    "    N, T, d = X_days.shape # n_trading_days, tokens_per_day, features_per_token\n",
    "\n",
    "    # initialize stack lists\n",
    "    Xc, yc = [], []\n",
    "\n",
    "    # slide rollwing window over dataset\n",
    "    for i in range(N - context_window + 1):\n",
    "        Xi = X_days[i:i+context_window].reshape(context_window * T, d)  # [seq_len, d]\n",
    "        yi = y_days[i + context_window - 1]  # label aligned to last input day → predict next day\n",
    "\n",
    "        # append sequence and length to lists\n",
    "        Xc.append(Xi)\n",
    "        yc.append(yi)\n",
    "\n",
    "    # return stacked sequences\n",
    "    return np.stack(Xc).astype(np.float32), np.array(yc, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf73123-c9bc-46e9-93b4-d63a96a41c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def mk_loader(X, y, bs=32, shuffle=False, num_workers=2):\n",
    "    x_t = torch.tensor(X, dtype=torch.float32)\n",
    "    y_t = torch.tensor(y, dtype=torch.float32)\n",
    "    ds = TensorDataset(x_t, y_t)\n",
    "    return DataLoader(ds, batch_size=bs, shuffle=shuffle, pin_memory=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "427efff0-58e7-4375-ba5b-0c61a5960fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolTransformerTiny(nn.Module):\n",
    "    def __init__(self, d_in, d_model=128, nhead=4, num_layers=3, p_drop=0.1, use_cls=True, ff_mult=4):\n",
    "        super().__init__()\n",
    "        # flag for CLS\n",
    "        self.use_cls = use_cls\n",
    "        # input embedding 14 features -> d_model space\n",
    "        self.embed = nn.Linear(d_in, d_model)\n",
    "\n",
    "        # define one encoding layer\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, \n",
    "                nhead=nhead, \n",
    "                dim_feedforward=ff_mult*d_model,\n",
    "                dropout=p_drop, \n",
    "                batch_first=True, \n",
    "                norm_first=True\n",
    "            )\n",
    "\n",
    "        # stack several encoder layers\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "        # CLS token\n",
    "        if use_cls:\n",
    "            self.cls = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "            # init with random values, becomes summary token\n",
    "            nn.init.normal_(self.cls, mean=0.0, std=0.02)\n",
    "\n",
    "        # predition head\n",
    "        self.head = nn.Sequential(\n",
    "            # stabilize final embedding\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Dropout(p_drop),\n",
    "            # next day log RV prediction\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "\n",
    "    # how does data flow through model\n",
    "    # B = batch size 32\n",
    "    # L = sequence length 550 tokens for 22 day context\n",
    "    # d_in = features per token, here 14\n",
    "    def forward(self, x):              # x: [B,L,d_in]\n",
    "        # apply embed projection\n",
    "        h = self.embed(x)\n",
    "        \n",
    "        if self.use_cls:\n",
    "            # now first token in each sequence is CLS\n",
    "            cls = self.cls.expand(h.size(0), -1, -1) \n",
    "            h = torch.cat([cls, h], dim=1)\n",
    "        # start encoder\n",
    "        h = self.encoder(h)\n",
    "        # if CLS, take first token for pred, else mean of tokens\n",
    "        pooled = h[:,0] if self.use_cls else h.mean(dim=1)\n",
    "\n",
    "        # one vec per seq\n",
    "        return self.head(pooled).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ddc6b1c-c0f8-4e81-b515-5ed36dd4d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(Xtr, ytr, Xva, yva, *, \n",
    "                d_model=128, nhead=4, num_layers=3,\n",
    "                batch_size=32, lr=1e-3, max_epochs=50):\n",
    "\n",
    "    # data loaders\n",
    "    train_loader = mk_loader(Xtr, ytr, bs=batch_size, shuffle=True)\n",
    "    val_loader   = mk_loader(Xva, yva, bs=batch_size, shuffle=False)\n",
    "\n",
    "    # create instance of Transformer, move to CUDA\n",
    "    model = VolTransformerTiny(d_in=Xtr.shape[-1], d_model=d_model, nhead=nhead, num_layers=num_layers).cuda()\n",
    "    # ADAM\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "    # loss function\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # \n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        \n",
    "        # --- train ---\n",
    "        # set model to training mode\n",
    "        model.train()\n",
    "        train_loss=0\n",
    "        n=0\n",
    "\n",
    "        # iterate over train batches\n",
    "        for xb,yb in tqdm(train_loader, desc=f\"Epoch {epoch}/{max_epochs}\", leave=False):\n",
    "            # move to CUDA\n",
    "            xb,yb=xb.cuda(),yb.cuda()\n",
    "            # reset gradients\n",
    "            opt.zero_grad()\n",
    "            # run forward pass\n",
    "            pred=model(xb)\n",
    "            loss=loss_fn(pred,yb)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # clip gradients if l2 > 1.0, prevent exploding gradients\n",
    "            # stable training\n",
    "            nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "            # update optimizer \n",
    "            opt.step()\n",
    "            # adjust loss, counter\n",
    "            train_loss+=loss.item()*xb.size(0)\n",
    "            n+=xb.size(0)\n",
    "        # compute average training loss\n",
    "        train_loss/=n\n",
    "\n",
    "        # --- val ---\n",
    "        # set model to evaluation mode\n",
    "        model.eval()\n",
    "        val_loss=0\n",
    "        n=0\n",
    "        # disable gradient computing, dont need backward validation\n",
    "        with torch.no_grad():\n",
    "            # iterate over validation\n",
    "            for xb,yb in val_loader:\n",
    "                # move to CUDA\n",
    "                xb,yb=xb.cuda(),yb.cuda()\n",
    "                # forward model pass\n",
    "                pred=model(xb)\n",
    "                # calculate loss\n",
    "                loss=loss_fn(pred,yb)\n",
    "                val_loss+=loss.item()*xb.size(0)\n",
    "                n+=xb.size(0)\n",
    "        # average loss\n",
    "        val_loss/=n\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | Train {train_loss:.6f} | Val {val_loss:.6f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f7c81c8-2f04-481e-bc98-c3497ad24e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ret_1m\n",
      "381    2516\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "out = preprocess_stock(df_ibm, patch_len=15, train_frac=0.7, val_frac=0.15)\n",
    "Xtr_ctx, ytr_ctx = stack_context_days(out[\"X_train\"], out[\"y_train\"], context_window=22)\n",
    "Xva_ctx, yva_ctx = stack_context_days(out[\"X_val\"],   out[\"y_val\"],   context_window=22)\n",
    "Xte_ctx, yte_ctx = stack_context_days(out[\"X_test\"],  out[\"y_test\"],  context_window=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f867b7e1-310e-4bd5-b244-c04205c6f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1739, 550, 14) (1739,)\n",
      "Val:   (356, 550, 14) (356,)\n",
      "Test:  (357, 550, 14) (357,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasbuhringer/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest: \u001b[39m\u001b[38;5;124m\"\u001b[39m, Xte_ctx\u001b[38;5;241m.\u001b[39mshape, yte_ctx\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# === 2) Train the model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mXtr_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytr_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXva_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myva_ctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnhead\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# === 3) Evaluate on validation and test\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(Xtr, ytr, Xva, yva, d_model, nhead, num_layers, batch_size, lr, max_epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m val_loader   \u001b[38;5;241m=\u001b[39m mk_loader(Xva, yva, bs\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# create instance of Transformer, move to CUDA\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mVolTransformerTiny\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mXtr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnhead\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# ADAM\u001b[39;00m\n\u001b[1;32m     13\u001b[0m opt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1065\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \n\u001b[1;32m   1051\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1065\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \n\u001b[1;32m   1051\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/cuda/__init__.py:363\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    361\u001b[0m     )\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    367\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# === 1) Sanity checks (optional)\n",
    "print(\"Train:\", Xtr_ctx.shape, ytr_ctx.shape)\n",
    "print(\"Val:  \", Xva_ctx.shape, yva_ctx.shape)\n",
    "print(\"Test: \", Xte_ctx.shape, yte_ctx.shape)\n",
    "\n",
    "# === 2) Train the model\n",
    "model = train_model(\n",
    "    Xtr_ctx, ytr_ctx, Xva_ctx, yva_ctx,\n",
    "    d_model=128, nhead=4, num_layers=3,\n",
    "    batch_size=32, lr=1e-3, max_epochs=50\n",
    ")\n",
    "\n",
    "# === 3) Evaluate on validation and test\n",
    "import torch, numpy as np\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "def eval_mse(X, y, bs=32):\n",
    "    loader = mk_loader(X, y, bs=bs, shuffle=False)\n",
    "    model.eval()\n",
    "    tot, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.cuda(), yb.cuda()\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            tot += loss.item() * xb.size(0)\n",
    "            n += xb.size(0)\n",
    "    return tot / max(1, n)\n",
    "\n",
    "val_mse = eval_mse(Xva_ctx, yva_ctx)\n",
    "te_mse  = eval_mse(Xte_ctx, yte_ctx)\n",
    "print(f\"Val MSE (logRV): {val_mse:.6f} | Test MSE (logRV): {te_mse:.6f}\")\n",
    "\n",
    "# === 4) Get test predictions (for saving/plots/metrics)\n",
    "def predict_array(X, bs=32):\n",
    "    loader = mk_loader(X, np.zeros(len(X), dtype=np.float32), bs=bs, shuffle=False)\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.cuda()\n",
    "            preds.append(model(xb).cpu().numpy())\n",
    "    return np.concatenate(preds, axis=0)\n",
    "\n",
    "yte_pred_log = predict_array(Xte_ctx)          # predicted log(RV)\n",
    "yte_true_log = yte_ctx                         # true log(RV)\n",
    "\n",
    "# Optional metrics on RV scale (QLIKE etc.)\n",
    "def qlike(pred_log, true_log):\n",
    "    pred = np.exp(pred_log); true = np.exp(true_log)\n",
    "    eps = 1e-12\n",
    "    ratio = np.clip(true / np.clip(pred, eps, None), eps, 1e12)\n",
    "    return float(np.mean(ratio - np.log(ratio) - 1.0))\n",
    "\n",
    "te_qlike = qlike(yte_pred_log, yte_true_log)\n",
    "print(f\"Test QLIKE (RV): {te_qlike:.6f}\")\n",
    "\n",
    "# === 5) (Optional) Save model\n",
    "import torch\n",
    "torch.save(model.state_dict(), \"vol_transformer_tiny.pt\")\n",
    "print(\"Saved to vol_transformer_tiny.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d2a810-0f06-4030-af16-e9b7dd01a42b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
