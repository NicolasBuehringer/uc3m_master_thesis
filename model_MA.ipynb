{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "264f4486-a777-40c7-a190-60d2486c9237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# seeds\n",
    "import random, os\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b63801-b2a8-4ffd-a2b8-62721e06629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ibm = pd.read_csv(\"data/ibm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aecd154b-a35e-490e-bb98-c7a0b9e341fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stock(df_raw, patch_len, train_frac, val_frac):\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # turn column to datetime, add helper day column\n",
    "    df[\"Date Time\"] = pd.to_datetime(df[\"Date Time\"])\n",
    "    df = df.sort_values(\"Date Time\")\n",
    "    df[\"day\"] = df[\"Date Time\"].dt.date\n",
    "\n",
    "    # minutely log returns withing days from close\n",
    "    # fill first minute with 0 returns\n",
    "    df[\"log_close\"] = np.log(df[\"Close\"].astype(float))\n",
    "    df[\"ret_1m\"] = df.groupby(\"day\")[\"log_close\"].diff().fillna(0.0)\n",
    "\n",
    "    # to check if each days actually has 381 return minutes\n",
    "    day_return_count = df.groupby(\"day\")[\"ret_1m\"].size()\n",
    "    print(day_return_count.value_counts())\n",
    "    bars_mode = int(day_return_count.mode().iat[0]) # get mode of return minutes across dataset\n",
    "\n",
    "    # compute daily RV\n",
    "    daily = (df.groupby(\"day\")[\"ret_1m\"]\n",
    "               .agg(rv=lambda x: np.sum(x**2))\n",
    "               .reset_index())\n",
    "    \n",
    "    # in case there is a zero RV, replace it with small value before log\n",
    "    daily[\"log_rv\"] = np.log(daily[\"rv\"].replace(0.0, 1e-12))\n",
    "    # label for inputs from day d is log_rv of day d+1\n",
    "    daily[\"log_rv_tplus1\"] = daily[\"log_rv\"].shift(-1)\n",
    "    \n",
    "    # create binary filter for days without valid target (prob just last day)\n",
    "    valid_days = daily.dropna(subset=[\"log_rv_tplus1\"])[\"day\"]\n",
    "    # remove them from both minute_df and daily_df\n",
    "    df = df[df[\"day\"].isin(valid_days)].copy()\n",
    "    daily = daily[daily[\"day\"].isin(valid_days)].reset_index(drop=True)\n",
    "\n",
    "    # create minute index in each day\n",
    "    df[\"idx_in_day\"] = df.groupby(\"day\").cumcount()\n",
    "\n",
    "    # flag if we want to patch\n",
    "    if patch_len == 1:\n",
    "        # No patching: 1-minute tokens\n",
    "        df[\"patch_id\"] = df[\"idx_in_day\"]\n",
    "    # patch index inside day\n",
    "    else:\n",
    "        df[\"patch_id\"] = (df[\"idx_in_day\"] // patch_len).astype(int)\n",
    "\n",
    "    # number of full patches per day (patches with len() == patch_len)\n",
    "    tokens_full = (bars_mode // patch_len) if patch_len > 1 else bars_mode\n",
    "\n",
    "     # collect aggregate information for each patch\n",
    "    g = df.groupby([\"day\", \"patch_id\"])\n",
    "    patch = g.agg(\n",
    "        r_sum=(\"ret_1m\", \"sum\"), # summed returns\n",
    "        r_abs=(\"ret_1m\", lambda x: np.abs(x).sum()), # summed absolute returns\n",
    "        r_sq =(\"ret_1m\", lambda x: np.sum(x**2)), # summed squared returns\n",
    "        hi   =(\"High\", \"max\"), # high\n",
    "        lo   =(\"Low\", \"min\"), # low\n",
    "        vol  =(\"Volume\", \"sum\"), # volumne\n",
    "        r_last=(\"ret_1m\", \"last\"), # last return\n",
    "        n    =(\"ret_1m\", \"size\"), # number of values (should be patch_len)\n",
    "    ).reset_index()\n",
    "\n",
    "    # drop the last not filled token\n",
    "    patch = patch[patch[\"patch_id\"] < tokens_full].copy()\n",
    "\n",
    "    # robust log range\n",
    "    # patch[\"range_hl\"] = np.log(patch[\"hi\"] / patch[\"lo\"].replace(0, np.nan)).replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "    # patch = patch.drop(columns=[\"hi\", \"lo\"])\n",
    "\n",
    "\n",
    "\n",
    "    # positional encoding\n",
    "    \n",
    "    # relative time-of-day encoding\n",
    "    if tokens_full > 1:\n",
    "        patch[\"pos\"] = patch[\"patch_id\"] / (tokens_full - 1)\n",
    "    else:\n",
    "        patch[\"pos\"] = 0.0\n",
    "    patch[\"pos_sin\"] = np.sin(2 * np.pi * patch[\"pos\"])\n",
    "    patch[\"pos_cos\"] = np.cos(2 * np.pi * patch[\"pos\"])\n",
    "\n",
    "    # calendar encoding\n",
    "    # create small calendar daily index\n",
    "    cal = patch[[\"day\"]].drop_duplicates().copy()\n",
    "    cal_dt = pd.to_datetime(cal[\"day\"])\n",
    "    \n",
    "    # Day of Week\n",
    "    dow = cal_dt.dt.weekday\n",
    "    cal[\"dow_sin\"] = np.sin(2 * np.pi * dow / 7.0)\n",
    "    cal[\"dow_cos\"] = np.cos(2 * np.pi * dow / 7.0)\n",
    "    \n",
    "    # Day of Month\n",
    "    dom = cal_dt.dt.day\n",
    "    cal[\"dom_sin\"] = np.sin(2 * np.pi * dom / 31.0)\n",
    "    cal[\"dom_cos\"] = np.cos(2 * np.pi * dom / 31.0)\n",
    "    \n",
    "    # Month of Year\n",
    "    moy = cal_dt.dt.month\n",
    "    cal[\"moy_sin\"] = np.sin(2 * np.pi * moy / 12.0)\n",
    "    cal[\"moy_cos\"] = np.cos(2 * np.pi * moy / 12.0)\n",
    "\n",
    "    # Month-end flag\n",
    "    cal[\"is_month_end\"] = cal_dt.dt.is_month_end.astype(np.int8)\n",
    "\n",
    "    # Attach calendar features to every patch in that day\n",
    "    patch = patch.merge(cal, on=\"day\", how=\"left\")\n",
    "\n",
    "    feat_cols = [\n",
    "        \"r_sum\", \"r_abs\", \"r_sq\", \"vol\", \"r_last\",     # patch stats\n",
    "        \"pos_sin\", \"pos_cos\",                                      # time-of-day\n",
    "        \"dow_sin\", \"dow_cos\", \"dom_sin\", \"dom_cos\", \"moy_sin\", \"moy_cos\",  # calendar\n",
    "        \"is_month_end\",                                            # flag (0/1)\n",
    "    ]\n",
    "\n",
    "\n",
    "    # ------------ BUILD X\n",
    "    # get unique trading days, patch has one row for each patch\n",
    "    day_list = sorted(patch[\"day\"].unique())\n",
    "    # initialize lists\n",
    "    X_days, y_days = [], []\n",
    "    # map each day to its target in a dict\n",
    "    label_map = dict(zip(daily[\"day\"], daily[\"log_rv_tplus1\"]))\n",
    "\n",
    "    # loop over trading days\n",
    "    for d in day_list:\n",
    "        # get all patches in one day sorted by time\n",
    "        day_p = patch[(patch[\"day\"] == d)].sort_values(\"patch_id\")\n",
    "        # skip days with not full patches\n",
    "        if day_p.shape[0] != tokens_full:\n",
    "            continue\n",
    "        # turn features into numpy array of [Patch_len, n_features]\n",
    "        # each Xi is a matrix representation of one trading day\n",
    "        Xi = day_p[feat_cols].to_numpy(dtype=np.float32)            # [T_patches, d_features]\n",
    "        # look up target for that day from dict\n",
    "        yi = float(label_map[d])                                    # scalar target (log RV)\n",
    "        X_days.append(Xi)\n",
    "        y_days.append(yi)\n",
    "\n",
    "    # stack data\n",
    "    X = np.stack(X_days, axis=0)            # [N_days, T_tokens, d_in]\n",
    "    y = np.array(y_days, dtype=np.float32)  # [N_days]\n",
    "\n",
    "\n",
    "\n",
    "    # ------------- Train - Val - Test - Split\n",
    "    # number of trading days in data set\n",
    "    N = X.shape[0]\n",
    "    # train split\n",
    "    n_tr = int(N * train_frac)\n",
    "    # val split\n",
    "    n_va = int(N * val_frac)\n",
    "\n",
    "    # comput split indices\n",
    "    idx_tr = slice(0, n_tr)\n",
    "    idx_va = slice(n_tr, n_tr + n_va)\n",
    "    idx_te = slice(n_tr + n_va, N)\n",
    "\n",
    "    # split data\n",
    "    X_train, y_train = X[idx_tr], y[idx_tr]\n",
    "    X_val, y_val = X[idx_va], y[idx_va]\n",
    "    X_test, y_test = X[idx_te], y[idx_te]\n",
    "\n",
    "    # ------------------- Scaling\n",
    "\n",
    "    # only use train data for mean and sd\n",
    "    # flattens across days and tokens\n",
    "    mu = X_train.reshape(-1, X_train.shape[-1]).mean(axis=0, keepdims=True)   # [1, d_in], mean of each feature\n",
    "    sd = X_train.reshape(-1, X_train.shape[-1]).std(axis=0, keepdims=True) + 1e-8 # sd of each feature\n",
    "\n",
    "    def scale(arr):\n",
    "        return (arr - mu) / sd\n",
    "\n",
    "    X_train = scale(X_train)\n",
    "    X_val = scale(X_val)\n",
    "    X_test = scale(X_test)\n",
    "    \n",
    "    return {\n",
    "        \"X_train\": X_train, \"y_train\": y_train,\n",
    "        \"X_val\":   X_val, \"y_val\":   y_val,\n",
    "        \"X_test\":  X_test, \"y_test\":  y_test,\n",
    "        \"meta\": {\n",
    "            \"tokens_per_day\": tokens_full,\n",
    "            \"feature_names\": feat_cols,\n",
    "            \"bars_per_day_mode\": bars_mode,\n",
    "            \"train_days\": int(n_tr),\n",
    "            \"val_days\": int(n_va),\n",
    "            \"test_days\": int(N - n_tr - n_va),\n",
    "            \"scaler_mean\": mu.astype(np.float32),\n",
    "            \"scaler_std\": sd.astype(np.float32),\n",
    "            \"patch_len_minutes\": patch_len,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "281a1a2e-d3b9-4627-8c9c-18837846e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_context_days(X_days, y_days, context_window=22):\n",
    "    N, T, d = X_days.shape # n_trading_days, tokens_per_day, features_per_token\n",
    "\n",
    "    # initialize stack lists\n",
    "    Xc, yc = [], []\n",
    "\n",
    "    # slide rollwing window over dataset\n",
    "    for i in range(N - context_window + 1):\n",
    "        Xi = X_days[i:i+context_window].reshape(context_window * T, d)  # [seq_len, d]\n",
    "        yi = y_days[i + context_window - 1]  # label aligned to last input day → predict next day\n",
    "\n",
    "        # append sequence and length to lists\n",
    "        Xc.append(Xi)\n",
    "        yc.append(yi)\n",
    "\n",
    "    # return stacked sequences\n",
    "    return np.stack(Xc).astype(np.float32), np.array(yc, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bf73123-c9bc-46e9-93b4-d63a96a41c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def make_loader(X, y, bs=32, shuffle=False, num_workers=2):\n",
    "    x_t = torch.tensor(X, dtype=torch.float32)\n",
    "    y_t = torch.tensor(y, dtype=torch.float32)\n",
    "    ds = TensorDataset(x_t, y_t)\n",
    "    return DataLoader(ds, batch_size=bs, shuffle=shuffle, pin_memory=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "427efff0-58e7-4375-ba5b-0c61a5960fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolTransformerTiny(nn.Module):\n",
    "    def __init__(self, d_in, d_model=128, nhead=4, num_layers=3, p_drop=0.1, use_cls=True, ff_mult=4):\n",
    "        super().__init__()\n",
    "        # flag for CLS\n",
    "        self.use_cls = use_cls\n",
    "        # input embedding 14 features -> d_model space\n",
    "        self.embed = nn.Linear(d_in, d_model)\n",
    "\n",
    "        # define one encoding layer\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, \n",
    "                nhead=nhead, \n",
    "                dim_feedforward=ff_mult*d_model,\n",
    "                dropout=p_drop, \n",
    "                batch_first=True, \n",
    "                norm_first=True\n",
    "            )\n",
    "\n",
    "        # stack several encoder layers\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "        # CLS token\n",
    "        if use_cls:\n",
    "            self.cls = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "            # init with random values, becomes summary token\n",
    "            nn.init.normal_(self.cls, mean=0.0, std=0.02)\n",
    "\n",
    "        # predition head\n",
    "        self.head = nn.Sequential(\n",
    "            # stabilize final embedding\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Dropout(p_drop),\n",
    "            # next day log RV prediction\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "\n",
    "    # how does data flow through model\n",
    "    # B = batch size 32\n",
    "    # L = sequence length 550 tokens for 22 day context\n",
    "    # d_in = features per token, here 14\n",
    "    def forward(self, x):              # x: [B,L,d_in]\n",
    "        # apply embed projection\n",
    "        h = self.embed(x)\n",
    "        \n",
    "        if self.use_cls:\n",
    "            # now first token in each sequence is CLS\n",
    "            cls = self.cls.expand(h.size(0), -1, -1) \n",
    "            h = torch.cat([cls, h], dim=1)\n",
    "        # start encoder\n",
    "        h = self.encoder(h)\n",
    "        # if CLS, take first token for pred, else mean of tokens\n",
    "        pooled = h[:,0] if self.use_cls else h.mean(dim=1)\n",
    "\n",
    "        # one vec per seq\n",
    "        return self.head(pooled).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ddc6b1c-c0f8-4e81-b515-5ed36dd4d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(Xtr, ytr, Xva, yva, *, \n",
    "                d_model=128, nhead=4, num_layers=3,\n",
    "                batch_size=32, lr=1e-3, max_epochs=50):\n",
    "\n",
    "    # data loaders\n",
    "    train_loader = mk_loader(Xtr, ytr, bs=batch_size, shuffle=True)\n",
    "    val_loader   = mk_loader(Xva, yva, bs=batch_size, shuffle=False)\n",
    "\n",
    "    # create instance of Transformer, move to CUDA\n",
    "    model = VolTransformerTiny(d_in=Xtr.shape[-1], d_model=d_model, nhead=nhead, num_layers=num_layers).cuda()\n",
    "    # ADAM\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "    # loss function\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # \n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        \n",
    "        # --- train ---\n",
    "        # set model to training mode\n",
    "        model.train()\n",
    "        train_loss=0\n",
    "        n=0\n",
    "\n",
    "        # iterate over train batches\n",
    "        for xb,yb in tqdm(train_loader, desc=f\"Epoch {epoch}/{max_epochs}\", leave=False):\n",
    "            # move to CUDA\n",
    "            xb,yb=xb.cuda(),yb.cuda()\n",
    "            # reset gradients\n",
    "            opt.zero_grad()\n",
    "            # run forward pass\n",
    "            pred=model(xb)\n",
    "            loss=loss_fn(pred,yb)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # clip gradients if l2 > 1.0, prevent exploding gradients\n",
    "            # stable training\n",
    "            nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "            # update optimizer \n",
    "            opt.step()\n",
    "            # adjust loss, counter\n",
    "            train_loss+=loss.item()*xb.size(0)\n",
    "            n+=xb.size(0)\n",
    "        # compute average training loss\n",
    "        train_loss/=n\n",
    "\n",
    "        # --- val ---\n",
    "        # set model to evaluation mode\n",
    "        model.eval()\n",
    "        val_loss=0\n",
    "        n=0\n",
    "        # disable gradient computing, dont need backward validation\n",
    "        with torch.no_grad():\n",
    "            # iterate over validation\n",
    "            for xb,yb in val_loader:\n",
    "                # move to CUDA\n",
    "                xb,yb=xb.cuda(),yb.cuda()\n",
    "                # forward model pass\n",
    "                pred=model(xb)\n",
    "                # calculate loss\n",
    "                loss=loss_fn(pred,yb)\n",
    "                val_loss+=loss.item()*xb.size(0)\n",
    "                n+=xb.size(0)\n",
    "        # average loss\n",
    "        val_loss/=n\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | Train {train_loss:.6f} | Val {val_loss:.6f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f7c81c8-2f04-481e-bc98-c3497ad24e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ret_1m\n",
      "381    2516\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "out = preprocess_stock(df_ibm, patch_len=15, train_frac=0.7, val_frac=0.15)\n",
    "Xtr_ctx, ytr_ctx = stack_context_days(out[\"X_train\"], out[\"y_train\"], context_window=22)\n",
    "Xva_ctx, yva_ctx = stack_context_days(out[\"X_val\"],   out[\"y_val\"],   context_window=22)\n",
    "Xte_ctx, yte_ctx = stack_context_days(out[\"X_test\"],  out[\"y_test\"],  context_window=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f867b7e1-310e-4bd5-b244-c04205c6f9c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xte_ctx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m             preds\u001b[38;5;241m.\u001b[39mappend(model(xb)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(preds, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m yte_pred_log \u001b[38;5;241m=\u001b[39m predict_array(\u001b[43mXte_ctx\u001b[49m)          \u001b[38;5;66;03m# predicted log(RV)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m yte_true_log \u001b[38;5;241m=\u001b[39m yte_ctx                         \u001b[38;5;66;03m# true log(RV)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# save model\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Xte_ctx' is not defined"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model = train_model(\n",
    "    Xtr_ctx, ytr_ctx, Xva_ctx, yva_ctx,\n",
    "    d_model=128, nhead=4, num_layers=3,\n",
    "    batch_size=32, lr=1e-3, max_epochs=50\n",
    ")\n",
    "\n",
    "# evaluate on validation and test\n",
    "import torch, numpy as np\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "#def eval_mse(X, y, bs=32):\n",
    "#    loader = mk_loader(X, y, bs=bs, shuffle=False)\n",
    "#    model.eval()\n",
    "#    tot, n = 0.0, 0\n",
    "#    with torch.no_grad():\n",
    "#        for xb, yb in loader:\n",
    "#            xb, yb = xb.cuda(), yb.cuda()\n",
    "#            pred = model(xb)\n",
    "#           loss = loss_fn(pred, yb)\n",
    "#            tot += loss.item() * xb.size(0)\n",
    "#            n += xb.size(0)\n",
    "#return tot / max(1, n)\n",
    "\n",
    "#val_mse = eval_mse(Xva_ctx, yva_ctx)\n",
    "#te_mse  = eval_mse(Xte_ctx, yte_ctx)\n",
    "#print(f\"Val MSE (logRV): {val_mse:.6f} | Test MSE (logRV): {te_mse:.6f}\")\n",
    "\n",
    "# get all test predictions\n",
    "def predict_array(X, bs=32):\n",
    "    loader = mk_loader(X, np.zeros(len(X), dtype=np.float32), bs=bs, shuffle=False)\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.cuda()\n",
    "            preds.append(model(xb).cpu().numpy())\n",
    "    return np.concatenate(preds, axis=0)\n",
    "\n",
    "yte_pred_log = predict_array(Xte_ctx)          # predicted log(RV)\n",
    "yte_true_log = yte_ctx                         # true log(RV)\n",
    "\n",
    "\n",
    "# save model\n",
    "import torch\n",
    "torch.save(model.state_dict(), \"vol_transformer_tiny.pt\")\n",
    "print(\"Saved to vol_transformer_tiny.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d2a810-0f06-4030-af16-e9b7dd01a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive baseline aligned with test set\n",
    "y_naive = np.roll(yte_true_log, 1)\n",
    "\n",
    "y_naive[0] = np.nan\n",
    "naive_mse = np.mean((y_naive - yte_true_log)**2)\n",
    "print(f\"Naive baseline Test MSE (logRV): {naive_mse:.6f}\")\n",
    "print(f\"Transformer Test MSE (logRV): {te_mse:.6f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(yte_true_log, label=\"True log RV\", color=\"black\")\n",
    "plt.plot(y_naive, label=\"Naive baseline (yesterday = tomorrow)\", linestyle=\"--\")\n",
    "plt.plot(yte_pred_log, label=\"Transformer\", alpha=0.8)\n",
    "plt.title(\"Volatility Forecasts (Test set)\")\n",
    "plt.xlabel(\"Test days\")\n",
    "plt.ylabel(\"log Realized Volatility\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
